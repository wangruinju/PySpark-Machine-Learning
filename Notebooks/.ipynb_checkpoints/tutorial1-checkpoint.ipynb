{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Here I quote the tutorial from [this great blog](https://weiminwang.blog/2016/06/09/pyspark-tutorial-building-a-random-forest-binary-classifier-on-unbalanced-dataset/) written by Dr. Weimin Wang ([Presentation](https://youtu.be/CdHuLGuU2c4)). The following code applys a random forest model with various sampling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier as RF\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import numpy as np\n",
    "import functools\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    " \n",
    "tableData = sqlContext.table('your_table_containing_products_feedback_information')\n",
    " \n",
    "cols_select = ['prod_price',\n",
    "               'prod_feat_1',\n",
    "               'prod_feat_2',\n",
    "               'cust_age',\n",
    "               'prod_feat_3',\n",
    "               'cust_region',\n",
    "               'prod_type',\n",
    "               'cust_sex',\n",
    "               'cust_title',\n",
    "               'feedback']\n",
    "df = tableData.select(cols_select).dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "responses = df.groupBy('feedback').count().collect()\n",
    "categories = [i[0] for i in responses]\n",
    "counts = [i[1] for i in responses]\n",
    " \n",
    "ind = np.array(range(len(categories)))\n",
    "width = 0.35\n",
    "plt.bar(ind, counts, width=width, color='r')\n",
    " \n",
    "plt.ylabel('counts')\n",
    "plt.title('Response distribution')\n",
    "plt.xticks(ind + width/2., categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    " \n",
    "binarize = lambda x: 'Negative' if x == 'Neutral' else x\n",
    " \n",
    "udfValueToCategory = udf(binarize, StringType())\n",
    "df = df.withColumn(\"binary_response\", udfConvertResponse(\"feedback\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_select = ['prod_price',\n",
    "               'prod_feat_1',\n",
    "               'prod_feat_2',\n",
    "               'cust_age',\n",
    "               'prod_feat_3',\n",
    "               'cust_region',\n",
    "               'prod_type',\n",
    "               'cust_sex',\n",
    "               'cust_title',\n",
    "               'feedback',\n",
    "               'binary_response']\n",
    " \n",
    "df = df.select(df.prod_price.cast('float'), # convert numeric cols (int or float) into a 'int' or 'float'\n",
    "               df.prod_feat_1.cast('float'),\n",
    "               df.prod_feat_2.cast('float'),\n",
    "               df.cust_age.cast('int'),\n",
    "               *cols_select[4:])\n",
    " \n",
    "df = df.fillna({'cust_region': 'NA', 'cust_title': 'NA', 'prod_type': 'NA'}) # fill in 'N/A' entries for certain cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    " \n",
    "COUNT_THRESHOLD = 150 # threshold to filter \n",
    " \n",
    "# create a temporary col \"count\" as counting for each value of \"prod_feat_3\"\n",
    "prodFeat3Count = df.groupBy(\"prod_feat_3\").count()\n",
    "df = df.join(prodFeat3Count, \"prod_feat_3\", \"inner\")\n",
    " \n",
    "def convertMinority(originalCol, colCount):\n",
    "    if colCount > COUNT_THRESHOLD:\n",
    "        return originalCol\n",
    "    else:\n",
    "        return 'MinorityCategory'\n",
    "createNewColFromTwo = udf(convertMinority, StringType())\n",
    "df = df.withColumn('prod_feat_3_reduced', createNewColFromTwo(df['prod_feat_3'], df['count']))\n",
    "df = df.drop('prod_feat_3')\n",
    "df = df.drop('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "column_vec_in = ['prod_feat_3_reduced', 'cust_region', 'prod_type', 'cust_sex', 'cust_title']\n",
    "column_vec_out = ['prod_feat_3_reduced_catVec','cust_region_catVec', 'prod_type_catVec','cust_sex_catVec',\n",
    "'cust_title_catVec']\n",
    " \n",
    "indexers = [StringIndexer(inputCol=x, outputCol=x+'_tmp')\n",
    "            for x in column_vec_in ]\n",
    " \n",
    "encoders = [OneHotEncoder(dropLast=False, inputCol=x+\"_tmp\", outputCol=y)\n",
    "for x,y in zip(column_vec_in, column_vec_out)]\n",
    "tmp = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "tmp = [i for sublist in tmp for i in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare labeled sets\n",
    "cols_now = ['prod_price',\n",
    "            'prod_feat_1',\n",
    "            'prod_feat_2',\n",
    "            'cust_age',\n",
    "            'prod_feat_3_reduced_catVec',\n",
    "            'cust_region_catVec',\n",
    "            'prod_type_catVec',\n",
    "            'cust_sex_catVec',\n",
    "            'cust_title_catVec']\n",
    "assembler_features = VectorAssembler(inputCols=cols_now, outputCol='features')\n",
    "labelIndexer = StringIndexer(inputCol='binary_response', outputCol=\"label\")\n",
    "tmp += [assembler_features, labelIndexer]\n",
    "pipeline = Pipeline(stages=tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allData = pipeline.fit(df).transform(df)\n",
    "allData.cache()\n",
    "trainingData, testData = allData.randomSplit([0.8,0.2], seed=0) # need to ensure same split for each time\n",
    "print(\"Distribution of Pos and Neg in trainingData is: \", trainingData.groupBy(\"label\").count().take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RF(labelCol='label', featuresCol='features',numTrees=200)\n",
    "fit = rf.fit(trainingData)\n",
    "transformed = fit.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "results = transformed.select(['probability', 'label'])\n",
    " \n",
    "## prepare score-label set\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "scoreAndLabels = sc.parallelize(results_list)\n",
    " \n",
    "metrics = metric(scoreAndLabels)\n",
    "print(\"The ROC score is (@numTrees=200): \", metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    " \n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in results_list]\n",
    "y_score = [i[0] for i in results_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    " \n",
    "RATIO_ADJUST = 2.0 ## ratio of pos to neg in the df_subsample\n",
    " \n",
    "counts = trainingData.select('binary_response').groupBy('binary_response').count().collect()\n",
    "higherBound = counts[0][1]\n",
    "TRESHOLD_TO_FILTER = int(RATIO_ADJUST * float(counts[1][1]) / counts[0][1] * higherBound)\n",
    " \n",
    "randGen = lambda x: randint(0, higherBound) if x == 'Positive' else -1\n",
    " \n",
    "udfRandGen = udf(randGen, IntegerType())\n",
    "trainingData = trainingData.withColumn(\"randIndex\", udfRandGen(\"binary_response\"))\n",
    "df_subsample = trainingData.filter(trainingData['randIndex'] < TRESHOLD_TO_FILTER)\n",
    "df_subsample = df_subsample.drop('randIndex')\n",
    " \n",
    "print(\"Distribution of Pos and Neg cases of the down-sampled training data are: \\n\", df_subsample.groupBy(\"label\").count().take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training and prediction\n",
    "rf = RF(labelCol='label', featuresCol='features',numTrees=200)\n",
    "fit = rf.fit(df_subsample)\n",
    "transformed = fit.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## results and evaluation\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "results = transformed.select(['probability', 'label'])\n",
    " \n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "scoreAndLabels = sc.parallelize(results_list)\n",
    " \n",
    "metrics = metric(scoreAndLabels)\n",
    "print(\"The ROC score is (@numTrees=200): \", metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble of Down-samplings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    " \n",
    "RATIO_ADJUST = 3.0 ## ratio of pos to neg in the df_subsample\n",
    "TOTAL_MODELS = 10\n",
    "total_results = None\n",
    "final_result = None\n",
    " \n",
    "#counts = trainingData.select('binary_response').groupBy('binary_response').count().collect()\n",
    "highestBound = counts[0][1]\n",
    "TRESHOLD_TO_FILTER = int(RATIO_ADJUST * float(counts[1][1]) / counts[0][1] * highestBound)\n",
    "## UDF\n",
    "randGen = lambda x: randint(0, highestBound) if x == 'Positive' else -1\n",
    "udfRandGen = udf(randGen, IntegerType())\n",
    " \n",
    "## ensembling\n",
    "for N in range(TOTAL_MODELS):\n",
    "    print(\"Round: \", N)\n",
    "    trainingDataIndexed = trainingData.withColumn(\"randIndex\", udfRandGen(\"binary_response\"))\n",
    "    df_subsample = trainingDataIndexed.filter(trainingDataIndexed['randIndex'] < TRESHOLD_TO_FILTER).drop('randIndex')\n",
    "    ## training and prediction\n",
    "    rf = RF(labelCol='label', featuresCol='features',numTrees=200)\n",
    "    fit = rf.fit(df_subsample)\n",
    "    transformed = fit.transform(testData)\n",
    "    result_pair = transformed.select(['probability', 'label'])\n",
    "    result_pair = result_pair.collect()\n",
    "    this_result = np.array([float(i[0][1]) for i in result_pair])\n",
    "    this_result = list(this_result.argsort().argsort() / (float(len(this_result) + 1)))\n",
    " \n",
    "    ## sum up all the predictions, and average to get final_result\n",
    "    if total_results is None:\n",
    "       total_results = this_result\n",
    "    else:\n",
    "       total_results = [i+j for i, j in zip(this_result, total_results)]\n",
    "    final_result = [i/(N+1) for i in total_results]\n",
    " \n",
    "    results_list = [(float(i), float(j[1])) for i, j in zip(final_result, result_pair)]\n",
    "    scoreAndLabels = sc.parallelize(results_list)\n",
    " \n",
    "    metrics = metric(scoreAndLabels)\n",
    "print(\"The ROC score is (@numTrees=200): \", metrics.areaUnderROC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
